
<!DOCTYPE html>

<html lang="english">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Welcome to Automotive IoT Sensor Fleet Deployment on AWS : An Architecture Proposal and Documentation&#39;s documentation! &#8212; Automotive IoT Sensor Fleet Deployment on AWS : An Architecture Proposal and Documentation 2023 documentation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/alabaster.css" />
    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="welcome-to-automotive-iot-sensor-fleet-deployment-on-aws-an-architecture-proposal-and-documentation-s-documentation">
<h1>Welcome to Automotive IoT Sensor Fleet Deployment on AWS : An Architecture Proposal and Documentation's documentation!<a class="headerlink" href="#welcome-to-automotive-iot-sensor-fleet-deployment-on-aws-an-architecture-proposal-and-documentation-s-documentation" title="Permalink to this heading">¶</a></h1>
<div class="toctree-wrapper compound">
</div>
</section>
<section id="indices-and-tables">
<h1>Indices and tables<a class="headerlink" href="#indices-and-tables" title="Permalink to this heading">¶</a></h1>
<ul class="simple">
<li><p><a class="reference internal" href="genindex.html"><span class="std std-ref">Index</span></a></p></li>
<li><p><a class="reference internal" href="py-modindex.html"><span class="std std-ref">Module Index</span></a></p></li>
<li><p><a class="reference internal" href="search.html"><span class="std std-ref">Search Page</span></a></p></li>
</ul>
<p># Archictecture Proposal for IoT Fleet Deployment, Storage and Analysis on AWS</p>
<p>![Architecture Diagram](../img/iots_jems_private.drawio(7).png)</p>
<hr class="docutils" />
<p>## <strong>I. Vehicle Gateway  parameterization</strong></p>
<p>_IoT Fleewise SDK as well as AWS IoT SDK and other softwares are installed on the computer board of each vehicle and parameterized according to the sensors/or and the computer board specs. Devices are given an individual ID, their model is tagged and they are attributed to a fleet if we want to group them and analyze them later <a href="#id9"><span class="problematic" id="id10">on_</span></a>.</p>
<p>—-----</p>
<p>[Demo auto aws Iot FleetWise](<a class="reference external" href="https://github.com/aws4embeddedlinux/demo-auto-aws-iotfleetwise">https://github.com/aws4embeddedlinux/demo-auto-aws-iotfleetwise</a>)
[Iot-device-simulator](<a class="reference external" href="https://github.com/aws-solutions/iot-device-simulator">https://github.com/aws-solutions/iot-device-simulator</a>)
[Iot Device Simulator](<a class="reference external" href="https://docs.aws.amazon.com/solutions/latest/iot-device-simulator/source-code.html">https://docs.aws.amazon.com/solutions/latest/iot-device-simulator/source-code.html</a>)
[Iot Fleetwise Edge Development Code](<a class="reference external" href="https://github.com/aws/aws-iot-fleetwise-edge/blob/main/docs/dev-guide/edge-agent-dev-guide.md#aws-iot-fleetwise-quick-start-demo">https://github.com/aws/aws-iot-fleetwise-edge/blob/main/docs/dev-guide/edge-agent-dev-guide.md#aws-iot-fleetwise-quick-start-demo</a>?sc_channel=EL&amp;sc_campaign=Live_Streaming_2022_vid&amp;sc_medium=YouTube&amp;sc_content=Pp_ZkcTlFX4&amp;sc_detail=INTERNET_OF_THINGS&amp;sc_country=US)
[AWS IoT Fleetwise Insights](<a class="reference external" href="https://aws.amazon.com/blogs/iot/generating-insights-from-vehicle-data-with-aws-iot-fleetwise-part1/">https://aws.amazon.com/blogs/iot/generating-insights-from-vehicle-data-with-aws-iot-fleetwise-part1/</a>)</p>
<p>—-----</p>
<p>### <strong>IoT Fleetwise vs IoT Core as primary data ingestion AWS service</strong></p>
<p>AWS IoT FleetWise (formerly AWS IoT FleetHub) and AWS IoT Core are two separate services designed for different purposes within the AWS IoT ecosystem. Here's a comparison of the two services with a focus on their similarities, differences, advantages, drawbacks, and complementarity in the context of managing a worldwide connected vehicle fleet sending terabytes of data per hour:</p>
<p><strong>1. AWS IoT FleetWise:</strong></p>
<ul class="simple">
<li><p>Purpose: FleetWise is a web application that enables you to monitor, track, and manage your connected vehicle fleet. It provides a centralized platform for fleet management, including real-time vehicle tracking, remote diagnostics, and over-the-air (OTA) updates.</p></li>
<li><p>Advantages: FleetWise simplifies fleet management by providing an easy-to-use interface and built-in features specifically tailored for connected vehicle fleets. It offers real-time monitoring, remote diagnostics, and OTA updates, which can help improve operational efficiency, reduce downtime, and save costs.</p></li>
<li><p>Drawbacks: FleetWise focuses primarily on fleet management tasks and doesn't provide the same level of flexibility, scalability, and integration with other AWS services as IoT Core.</p></li>
</ul>
<p><strong>2. AWS IoT Core:</strong>
* Purpose: IoT Core is a managed cloud service that enables secure and reliable communication between IoT devices and cloud applications. It supports billions of devices and trillions of messages while ensuring low latency and high throughput. IoT Core provides device connectivity, authentication, authorization, device management, and integration with other AWS services.
* Advantages: IoT Core offers robust device connectivity, security, and integration with other AWS services. It is highly scalable and supports a wide range of protocols, making it suitable for managing large connected vehicle fleets sending terabytes of data per hour. IoT Core's rules engine and integration with services like Kinesis, Lambda, and S3 enable efficient data processing, analytics, and storage.
* Drawbacks: IoT Core doesn't provide a dedicated interface for fleet management tasks like FleetWise. While it is possible to build custom fleet management applications using IoT Core, it requires more development effort compared to using the out-of-the-box features provided by FleetWise.</p>
<p>Complementarity : IoT FleetWise and IoT Core can be complementary in managing a connected vehicle fleet. IoT Core can be used to handle device connectivity, data ingestion, and processing, while FleetWise can serve as the fleet management interface for monitoring, tracking, and managing your connected vehicles. By integrating both services, you can leverage the strengths of each service to create a comprehensive solution for your connected vehicle fleet.</p>
<p>Choosing the best one: If your primary requirement is an easy-to-use fleet management solution with built-in features tailored for connected vehicle fleets, AWS IoT FleetWise might be the better choice.</p>
<p>However, if you need a highly scalable and flexible IoT platform that can handle billions of devices, trillions of messages, and integrate with other AWS services for data processing and analytics, AWS IoT Core is the more suitable option.</p>
<p>### <strong>AWS IoT FleetWise Edge Agent</strong></p>
<p>Edge Agent software can be installed on sensors embedded in vehicles to collect, process, and analyze sensor data in real-time. It is designed to work in tandem with AWS IoT FleetWise and provide edge computing capabilities for vehicle IoT sensor data processing before transmission to the cloud.</p>
<p>To transfer the data over to the cloud, Edge Agent uses AWS IoT Core. It provides secure, scalable, and cost-efficient MQTT/TLS connectivity. Route the vehicle data to the purpose-built database or storage services. At the time of publishing of this blog, AWS IoT FleetWise supports Amazon Timestream service. Amazon S3 will be supported after GA.</p>
<p>The benefits of using AWS IoT FleetWise Edge Agent are as follows:</p>
<ul class="simple">
<li><p>Real-time processing: Edge Agent allows for real-time processing of IoT sensor data, allowing businesses to quickly detect issues and make decisions more rapidly.</p></li>
<li><p>Low latency: Edge processing reduces latency by reducing the time required to transmit data to the cloud and receive a response.</p></li>
<li><p>Reduced data transmission costs: By processing data at the edge, it is possible to reduce the amount of data that needs to be transmitted to the cloud, thereby reducing data transmission costs.</p></li>
<li><p>Security: Edge Agent can be used to implement additional security measures, such as encrypting data in transit.</p></li>
</ul>
<p>Compared to data processing on the onboard computer of vehicles using a microcontroller optimized for connected vehicles, using AWS IoT FleetWise and Edge Agent offers several advantages, such as:</p>
<ul class="simple">
<li><p>Scalability: AWS IoT FleetWise is designed to handle large-scale vehicle fleets and can easily integrate with other AWS services for data storage, processing, and analysis.</p></li>
<li><p>Flexibility: Businesses can easily add new types of sensors and vehicle data to their IoT architecture without having to modify the entire processing infrastructure.</p></li>
<li><p>Advanced analysis: Businesses can use advanced analysis tools such as Amazon SageMaker to perform predictive analysis on IoT sensor data and make more informed decisions.</p></li>
</ul>
<p>In summary, using AWS IoT FleetWise and Edge Agent enables real-time processing of vehicle IoT sensor data, reducing latency and data transmission costs while providing advanced analysis and security features. These benefits, combined with the scalability and flexibility of AWS IoT, make it an attractive solution for businesses managing large fleets of connected vehicles.</p>
<p>Fleetwise is an AWS service that securely manages IoT device certificates and identities. By using Fleetwise, you can create, manage, and revoke certificates for IoT devices, greatly simplifying management of large-scale IoT devices.</p>
<p>To use Fleetwise with a Raspberry Pi, follow these steps:</p>
<ol class="arabic simple">
<li><p>Create an instance of the AWS Private Certificate Authority (PCA) service using AWS Certificate Manager (ACM). This step creates a private certificate authority that can be used to sign IoT device certificates.</p></li>
<li><p>Create a Thing Group in AWS IoT Core for each vehicle fleet. Thing Groups are collections of IoT devices that share common characteristics.</p></li>
<li><p>Create device certificates for each Raspberry Pi using Fleetwise. Certificates can be created manually or automatically using a certificate request process.</p></li>
<li><p>Associate certificates with the corresponding Things in AWS IoT Core.</p></li>
<li><p>Configure the Raspberry Pis to use certificates to authenticate with AWS IoT Core. This can be done using the AWS IoT SDK for Python.</p></li>
</ol>
<p>Using Fleetwise, you can also manage access permissions for IoT devices. You can create access policies and associate them with Thing Groups to control access to AWS resources, such as Kinesis streams or DynamoDB databases.</p>
<p>### <strong>Problems solved with Fleetwise</strong></p>
<ol class="arabic simple">
<li><p>Implementation complexity due to proprietary data formats : The variety of data formats leads to high complexity of systems needed to analyze vehicle-wide and fleet-wide vehicle data. This complexity results in a high implementation and maintenance effort, often preventing or slowing down implementation of data-driven use cases.</p></li>
<li><p>AWS IoT FleetWise helps to reduce data volume by providing intelligent data filtering capabilities. With AWS IoT FleetWise, you can reduce data volume in two ways.</p></li>
<li><p>First, you can configure the vehicle to collect only the signals, that are required for the purpose of your use cases. Second, you can configure AWS IoT FleetWise to collect the signals only under certain conditions. Examples for such conditions are scheduled collection (e.g., only between 1PM and 2PM on a specific date) or condition-based collection (e.g., only when battery temperature is above the threshold).</p></li>
<li><p>The AWS IoT FleetWise Edge Agent software running in vehicles uses campaigns to decide how to collect and transfer data to the cloud. You create campaigns in the cloud. After you or your team approve campaigns, AWS IoT FleetWise automatically deploys them to vehicles.</p></li>
</ol>
<p>## <strong>II. Security, Communication and Governance</strong></p>
<p>_A  PKI (Public Key Infrastructure) must be established to ensure the authenticity of both the clients (devices) and servers (AWS services and enterprise servers) and the encryption of the communications. Both attestation ( verify authenticity) and operational (establish secure communication) must be issued by a Root Authority, managed by AWS Private CA to ensure communication between IoT devices and IoT Core._</p>
<p>—---------------------------------</p>
<p>### <strong>Certificate Signing Request :</strong></p>
<p>CSR stands for Certificate Signing Request, which is a message sent by a client, such as a Vehicle Gateway, to a Certificate Authority (CA) or Certificate Broker requesting a digital certificate. In the context of IoT security, a digital certificate is used to authenticate devices and ensure secure communication between them.</p>
<p>When a Vehicle Gateway sends a CSR to a Certificate Broker, it includes information about the device, such as its unique identifier, public key, and other metadata. The Certificate Broker then uses this information to issue a digital certificate that is specific to the Vehicle Gateway. This digital certificate is signed by the Certificate Authority, indicating that the certificate can be trusted.</p>
<p>Once the Vehicle Gateway receives the digital certificate from the Certificate Broker, it can use this certificate to establish a secure connection with other devices or services, such as an IoT cloud platform or other vehicles in a fleet. The certificate serves as proof of identity and authenticity, ensuring that only authorized devices can access the system.</p>
<p>In summary, the CSR communication between a Vehicle Gateway and a Certificate Broker is an important step in the process of establishing secure communication between IoT devices. It enables the Vehicle Gateway to obtain a digital certificate that can be used to authenticate the device and establish secure connections with other devices and services.</p>
<p>### <strong>Attestation and operational certificate</strong></p>
<p>Attestation and operational certificates are two different types of certificates used in the context of IoT security.</p>
<p><strong>Attestation certificates</strong> are used to verify the identity and authenticity of a device. They are typically used during the device onboarding process, where a device sends its identity information to a certificate authority or certificate broker. The certificate authority or broker then issues an attestation certificate that verifies the device's identity and ensures that it is trusted by the system. Attestation certificates are typically used in the initial stages of device provisioning and are only valid for a limited time.</p>
<p><strong>Operational certificates</strong>, on the other hand, are used to <a href="#id1"><span class="problematic" id="id2">**</span></a>establish secure communication channels between devices and services once the device is operational. They are used to encrypt data sent between devices and ensure that it can only be read by authorized parties. Operational certificates are typically issued by a certificate authority or broker and can be valid for a longer period of time than attestation certificates.</p>
<p>In summary, attestation certificates are used to verify the identity and authenticity of a device during the onboarding process, while operational certificates are used to establish secure communication channels between devices and services once the device is operational. Both types of certificates play a critical role in securing IoT devices and ensuring that they can communicate securely and reliably with other devices and services in the IoT ecosystem.</p>
<p>### <strong>AWS Private CA</strong></p>
<p>AWS Private CA (Certificate Authority) is a fully-managed private certificate authority service that enables you to create and manage private certificates for your organization's infrastructure and applications.</p>
<p>With AWS Private CA, you can create a private CA hierarchy that is managed by AWS, which allows you to issue X.509 digital certificates for use within your organization. The private CA can be used to issue certificates for internal systems, devices, applications, and services, as well as for client and server authentication.</p>
<p>AWS Private CA provides a highly secure, scalable, and highly available solution for managing digital certificates. You can use it to issue, renew, and revoke certificates, and to manage the entire lifecycle of your organization's digital certificates.</p>
<p>Some of the key features of AWS Private CA include:</p>
<ol class="arabic simple">
<li><p>Integration with AWS services: AWS Private CA integrates with other AWS services such as Amazon S3, AWS Certificate Manager, AWS Key Management Service (KMS), AWS CloudTrail, and AWS Identity and Access Management (IAM).</p></li>
<li><p>Automation: AWS Private CA can automate the certificate issuance process, making it easy to manage large-scale deployments of certificates.</p></li>
<li><p>Security: AWS Private CA is highly secure, using encryption to protect your private keys and providing options for revocation and renewal of certificates.</p></li>
<li><p>Scalability: AWS Private CA is highly scalable and can support a large number of certificates for your organization.</p></li>
<li><p>Compliance: AWS Private CA supports industry-standard security protocols and compliance frameworks such as HIPAA, PCI DSS, and SOC.</p></li>
</ol>
<p>Overall, AWS Private CA provides a highly secure and scalable solution for managing digital certificates for your organization, enabling you to issue and manage certificates for your internal systems, devices, applications, and services.</p>
<p>### <strong>Certificate broker and root certificate authority</strong></p>
<p>A <strong>certificate broker</strong> and a <strong>root certificate authority</strong> (CA) are both involved in managing digital certificates in a public key infrastructure (PKI) system, but they have different roles and responsibilities.</p>
<p>A <strong>root CA</strong> is the top-level CA in a PKI hierarchy, responsible for issuing and managing root certificates. Root certificates are used to establish trust for digital certificates issued by lower-level CAs, also known as subordinate CAs. Root certificates are typically distributed to devices and web browsers to enable them to trust the digital certificates issued by the subordinate CAs.</p>
<p>A certificate broker, on the other hand, is a service that provides a central point of control for the issuance and management of digital certificates. It acts as an intermediary between clients and CAs, handling the communication and management of certificates on behalf of the clients. Certificate brokers can issue certificates from multiple CAs and provide additional services such as certificate revocation and renewal.</p>
<p>In summary, a root CA is responsible for issuing and managing root certificates, which establish trust for digital certificates issued by subordinate CAs. A certificate broker is an intermediary service that manages the issuance and management of digital certificates on behalf of clients, providing a centralized point of control and additional services such as certificate revocation and renewal.</p>
<p>### <strong>Operational and Subordinate certificates</strong></p>
<p>Operational certificates and subordinate certificates serve different purposes within a Public Key Infrastructure (PKI). Here are the key differences between them:</p>
<p><strong>Operational Certificates:</strong></p>
<ol class="arabic simple">
<li><p>Purpose: Operational certificates are issued to servers, devices, or individuals to authenticate, secure communications, and establish encrypted connections. They are used for operations like SSL/TLS connections, code signing, email encryption, and document signing.</p></li>
<li><p>Trust level: Operational certificates are issued by an intermediate or root Certificate Authority (CA) and are trusted based on the chain of trust established by the PKI hierarchy.</p></li>
<li><p>Validity period: The validity period of operational certificates is typically shorter, ranging from months to a few years. This is because operational certificates are more likely to be exposed to security threats, so shorter validity periods help mitigate risks.</p></li>
<li><p>Revocation: If an operational certificate is compromised or the certificate's private key is lost, it can be revoked by the issuing CA, and a new certificate will be issued.</p></li>
<li><p>Scope: Operational certificates are usually limited to specific domains, devices, or individuals.</p></li>
</ol>
<p><strong>Subordinate Certificates:</strong></p>
<ol class="arabic simple">
<li><p>Purpose: Subordinate certificates are issued to intermediate CAs, allowing them to issue operational certificates on behalf of the root CA. This reduces the risk and workload of the root CA, as it does not need to directly issue all operational certificates.</p></li>
<li><p>Trust level: Subordinate certificates are issued by a root CA or another intermediate CA higher in the PKI hierarchy. They are used to establish the chain of trust between the root CA and the operational certificates issued by the intermediate CA.</p></li>
<li><p>Validity period: Subordinate certificates usually have longer validity periods compared to operational certificates, as they represent the trust in an intermediate CA rather than an individual device or service.</p></li>
<li><p>Revocation: Revoking a subordinate certificate is a significant event, as it impacts all operational certificates issued by the affected intermediate CA. This is typically done in case of a compromise or if the intermediate CA's private key is lost.</p></li>
<li><p>Scope: Subordinate certificates are not limited to specific domains, devices, or individuals. Instead, they authorize an intermediate CA to issue operational certificates within the scope defined by the root CA.</p></li>
</ol>
<p>In the case of AWS Private CA, a subordinate certificate allows AWS to act as an intermediate CA and issue operational certificates on behalf of the root CA. This enables customers to create and manage their own private PKI hierarchy within AWS, ensuring secure communications within their cloud infrastructure.</p>
<p><strong>mTLS</strong> stands for Mutual Transport Layer Security, which is a type of secure communication protocol used to authenticate and encrypt data transmission between two parties over a network. In mTLS, both the client and the server are required to present digital certificates to each other to verify their identities and establish a secure connection.</p>
<p>When a client initiates a connection to a server using mTLS, it presents its own digital certificate to the server as proof of its identity. The server then verifies the authenticity of the client's certificate by checking its digital signature against a trusted certificate authority (CA) or a certificate revocation list (CRL).</p>
<p><strong>MQTT</strong> :</p>
<p>## <strong>III. IoT Core for raw Data Ingestion</strong></p>
<p>_IoT Core centralizes the datas coming from the vehiclesas it is designed to support billions of devices and trillions of messages while ensuring low latency and high throughput. Device authorization and authentification is ensured, communication is established through the MQTT protocol and easily integrates and pass the datas to our kinesis firehorse service, our kinesis stream and IoT Defender._</p>
<p>—------------------</p>
<p>### <strong>AWS IoT Core</strong></p>
<p>is a managed cloud service that enables secure and reliable communication between Internet of Things (IoT) devices and cloud applications. It is designed to support billions of devices and trillions of messages while ensuring low latency and high throughput. Here's an exhaustive description of IoT Core and its advantages when dealing with thousands of connected vehicles sending terabytes of data per hour:</p>
<ol class="arabic simple">
<li><p>Device connectivity: IoT Core supports MQTT, WebSockets, and HTTP/2 protocols, enabling seamless connectivity between various IoT devices, including connected vehicles, and the cloud.</p></li>
<li><p>Device authentication and authorization: IoT Core provides robust authentication and authorization mechanisms, including X.509 certificate-based, token-based, and custom authorizers. This ensures secure communication between connected vehicles and the cloud.</p></li>
<li><p>Device registry and management: IoT Core allows you to create and manage a registry of devices, providing metadata and indexing capabilities for better fleet management of connected vehicles.</p></li>
<li><p>Device shadow: IoT Core maintains a virtual representation (shadow) of each connected device, allowing you to track and synchronize the device's state even when it's offline. This is particularly useful for managing vehicle fleets with intermittent connectivity.</p></li>
<li><p>Rules engine: IoT Core features a rules engine that processes and routes data to other AWS services based on user-defined rules. You can use this to transform and process terabytes of data received from connected vehicles, trigger alerts, or store data in databases.</p></li>
<li><p>Scalability and performance: IoT Core is designed to handle billions of devices and trillions of messages, ensuring low latency and high throughput. This makes it an ideal solution for managing thousands of connected vehicles sending terabytes of data per hour.</p></li>
<li><p>Integration with other AWS services: IoT Core integrates with other AWS services like Amazon Kinesis, AWS Lambda, Amazon S3, and Amazon DynamoDB, allowing you to process, analyze, and store the data collected from connected vehicles in a scalable and cost-effective manner.</p></li>
<li><p>Security and compliance: IoT Core is built on AWS's secure and compliant infrastructure, ensuring data privacy and meeting industry standards like ISO 27001, ISO 27017, ISO 27018, and SOC.</p></li>
<li><p>Pay-as-you-go pricing: IoT Core has a pay-as-you-go pricing model, allowing you to pay only for the number of messages sent and received, with no upfront costs.</p></li>
</ol>
<p>Advantages in the context of thousands of IoT-connected vehicles sending terabytes of data per hour:</p>
<ol class="arabic simple">
<li><p>Real-time data processing: IoT Core can process and route terabytes of data from connected vehicles to appropriate AWS services in real-time, enabling faster decision-making and better fleet management.</p></li>
<li><p>Improved operational efficiency: IoT Core's device management, rules engine, and device shadows enable better monitoring and control of connected vehicles, improving operational efficiency and reducing costs.</p></li>
<li><p>Enhanced security: Secure authentication and authorization mechanisms ensure that the data exchange between connected vehicles and the cloud is protected from unauthorized access and tampering.</p></li>
<li><p>Scalability: IoT Core can handle the massive volume of data generated by thousands of connected vehicles without compromising performance or reliability.</p></li>
<li><p>Seamless integration: Easy integration with other AWS services enables a wide range of use cases, from real-time analytics and machine learning to long-term storage and archiving.</p></li>
<li><p>Cost-effective: Pay-as-you-go pricing ensures that you only pay for the services you use, reducing the overall cost of managing a large fleet of connected vehicles.</p></li>
</ol>
<p>### <strong>IOT Core vs Lambda</strong></p>
<p>AWS IoT Core is designed specifically for IoT use cases, and provides a number of features and capabilities that make it well-suited for handling high volumes of data from large fleets of connected devices.</p>
<p>One of the key benefits of using IoT Core for handling IoT data is its ability to handle large volumes of data with low latency. IoT Core provides scalable message ingestion and data processing, enabling you to handle large volumes of data from your fleet of connected devices in real-time. IoT Core also provides support for a wide range of IoT protocols, including MQTT, HTTPS, and WebSocket, making it easy to connect and manage large fleets of devices.</p>
<p>Another key benefit of using IoT Core is its integration with other AWS services, such as Kinesis, S3, and Lambda. This enables you to build scalable and flexible IoT solutions using a combination of different AWS services, depending on your specific requirements.</p>
<p>While Lambda can also be used to handle IoT data, it may not be the best choice for handling high volumes of data from large fleets of connected devices. Lambda functions are designed to be stateless and short-lived, making them better suited for handling small to medium volumes of data in short bursts. In contrast, IoT Core is designed to handle large volumes of data over longer periods of time, making it better suited for handling high volumes of data from large fleets of connected devices.</p>
<p>### <strong>AWS IoT Device Management</strong></p>
<p>provides the ability to onboard, configure, and manage IoT devices at scale. It allows you to organize your devices into groups, manage device configurations, and perform firmware over-the-air (OTA) updates. In your use case, you can use IoT Device Management to register your connected vehicles as devices, group them by fleet or other criteria, and manage their configurations and OTA updates. For example, you can remotely update the firmware of all the devices in a particular fleet to ensure that they have the latest software patches and security updates.</p>
<p>### <strong>IoT Defender</strong></p>
<p>### <strong>MQTT vs HTTPS Protocols</strong></p>
<p>## <strong>IV. Datalake/Warehouse creation and Batch Analytics</strong></p>
<p>_The raw datas coming from the IoT Core are passed to Kinesis Firehorse in order to be stored to a primary raw data S3 bucket in csv. format if needed. Then the raw datas can eventually be processed and refined with EMR and stored for long term usage and query to Redshift which gives data scientist flexibility to analyze and process the very raw datas and be extracted as batch with vehicle informations._</p>
<p>### <strong>Possible improvements</strong></p>
<ol class="arabic simple">
<li><p>AWS Glue DataBrew: Instead of using AWS Glue for data processing and refining, you can use AWS Glue DataBrew. DataBrew is a visual data preparation tool that allows you to clean and transform data without writing code. It can help simplify the data processing step for users who are not comfortable with writing ETL scripts in AWS Glue.</p></li>
<li><p>AWS IoT Analytics: If you're looking for a more integrated solution specifically for IoT data, you can use AWS IoT Analytics. This service enables you to collect, process, enrich, and analyze IoT data. You can replace the combination of IoT Core rules, Kinesis Data Firehose, and AWS Glue with AWS IoT Analytics for a more streamlined solution. IoT Analytics can store processed data in Amazon S3 and can be integrated with Amazon Redshift for further analysis.</p></li>
<li><p>Redshift Spectrum: If you're using Amazon Redshift for data analysis, consider using Redshift Spectrum to query data directly from S3 without the need to load it into Redshift. This can save time and resources spent on loading data into the Redshift cluster, especially if you're only querying a subset of the data. With Redshift Spectrum, you can keep your data in S3 and run SQL queries on it directly from your Redshift cluster.</p></li>
</ol>
<p>—----------------</p>
<p>### <strong>Data Warehouse vs DataLake</strong></p>
<blockquote>
<div><p>Data warehouses and data lakes are both designed for storing and managing large volumes of data, but they serve different purposes and have unique characteristics. Here are the key differences between them:</p>
</div></blockquote>
<p>1. Data type and structure:
* Data Warehouse: Stores structured data in a schema, typically using a relational database management system (RDBMS). Data is transformed and cleaned before being loaded into the warehouse, making it suitable for analysis and reporting.
* Data Lake: Stores data in its raw, native format, including structured, semi-structured, and unstructured data. This allows for greater flexibility in the types of data that can be ingested and stored.
2. Data processing:
* Data Warehouse: Follows the ETL (Extract, Transform, Load) process, where data is extracted from source systems, transformed into a common format, and loaded into the warehouse.
* Data Lake: Follows the ELT (Extract, Load, Transform) process, where data is first loaded in its raw format and then transformed as needed during analysis.
3. Schema design:
* Data Warehouse: Uses a schema-on-write approach, where data is structured into a predefined schema before being loaded. This ensures that the data is consistent and ready for analysis.
* Data Lake: Uses a schema-on-read approach, where data is stored in its raw format and a schema is applied only when it's read for analysis. This allows for more flexibility and adaptability to changing business requirements.
4. Storage:
* Data Warehouse: Typically built on traditional RDBMS technologies, which store data in tables and use a columnar or row-based storage format.
* Data Lake: Built on distributed storage systems like Hadoop Distributed File System (HDFS) or cloud-based object storage like Amazon S3, which can store large volumes of data at a lower cost than traditional RDBMS.
5. Querying and analysis:
* Data Warehouse: Optimized for fast, complex queries and aggregations on structured data. Supports SQL and other query languages for data analysis and reporting.
* Data Lake: Supports a variety of data processing and analytics tools, including batch processing, real-time processing, machine learning, and advanced analytics. Requires additional tools and services to query and analyze the data.
5. Use cases:
* Data Warehouse: Ideal for structured data analysis, reporting, and business intelligence use cases, where data consistency and query performance are critical.
* Data Lake: Suited for a broader range of use cases, including big data processing, advanced analytics, machine learning, and artificial intelligence, as well as handling diverse and evolving data sources.</p>
<p>In summary, data warehouses are optimized for storing and analyzing structured data, while data lakes are designed to store and process diverse data types at scale. The choice between a data warehouse and a data lake depends on your specific data storage, processing, and analysis requirements.</p>
<p>### <strong>Batch Data Pipeline</strong></p>
<ol class="arabic simple">
<li><p><strong>IoT Fleetwise</strong>: Collects data from your fleet of vehicles and securely sends it to AWS IoT Core.</p></li>
<li><p><strong>IoT Core</strong>: Processes and routes the data to the appropriate AWS service, in this case, Amazon Kinesis Data Firehose.</p></li>
<li><p><strong>Kinesis Data Firehose</strong>: Ingests, buffers, and delivers the data to Amazon S3. You can configure Kinesis Data Firehose to convert the incoming data format (e.g., JSON) into CSV before storing it in S3.</p></li>
<li><p><strong>S3</strong>: Stores the CSV-formatted data from Kinesis Data Firehose, making it available for further processing by Amazon EMR.</p></li>
<li><p><strong>EMR</strong>: Amazon EMR can be used to process the data in S3 using distributed data processing frameworks like Apache Spark or Apache Hadoop. You can write a custom application to process time series data, calculate features, and aggregate results into batch CSV files. Additionally, you can generate the XML file describing the data schema as part of this process.</p></li>
<li><p><strong>Redshift</strong>: If you want to store the processed data in Amazon Redshift, you can load the CSV files generated by the EMR processing step into the data warehouse for further analysis, querying, or reporting.</p></li>
</ol>
<p>If we only need to store the processed data as CSV files in S3 and do not require the data warehouse capabilities of Amazon Redshift, you can skip the Redshift step. Instead, write the processed CSV files and XML schema file back to S3 after processing with EMR. This will make the processed data available for further analysis or consumption by other applications.</p>
<p>### <strong>Kinesis Data Streams et Kinesis Data Firehose</strong></p>
<p>Deux services AWS qui permettent l'ingestion et le traitement de flux de données en temps réel, mais ils ont des différences clés en termes de fonctionnalités et d'utilisation.</p>
<p>Kinesis Data Streams est un service de flux de données en temps réel qui permet l'ingestion de données en continu, la persistance de données pendant 24 heures ou plus, le traitement de données à l'aide de Lambda ou d'Amazon Kinesis Client Library, et la réplication de données à travers plusieurs régions AWS. Kinesis Data Streams permet également de traiter des données en temps réel à l'aide d'outils tels que Apache Storm, Apache Flink ou Spark Streaming. Il est principalement utilisé pour les cas d'usage nécessitant une faible latence de traitement des données, tels que les analyses de flux de clics, les analyses de fraudes ou les alertes en temps réel.</p>
<p>Kinesis Data Firehose, en revanche, est un service de livraison de flux de données en temps réel qui permet d'acheminer les données vers des destinations telles qu'Amazon S3, Amazon Redshift, Amazon Elasticsearch ou Amazon Kinesis Data Analytics. Kinesis Data Firehose effectue également des transformations de données en temps réel à l'aide de Lambda ou de logiciels tiers, et prend en charge la réplication de données à travers plusieurs régions AWS. Kinesis Data Firehose est principalement utilisé pour les cas d'usage qui nécessitent un traitement simplifié des données en temps réel, tels que l'archivage de données en temps réel ou l'analyse de logs en temps réel.</p>
<p>En résumé, Kinesis Data Streams est un service de flux de données en temps réel pour l'ingestion, le traitement et la persistance de données en continu, tandis que Kinesis Data Firehose est un service de livraison de flux de données en temps réel pour l'acheminement et la transformation de données vers des destinations de stockage ou d'analyse en temps réel.</p>
<p>### <strong>Glue vs Lambda to perform final data cleaning job</strong></p>
<p>AWS Glue is a fully managed extract, transform, and load (ETL) service that can automate the process of converting the processed data to a CSV format. It can also generate metadata and data catalog, which makes it easier to discover and query the data later. Glue can also handle schema evolution and can adapt to changes in the schema over time.</p>
<p>On the other hand, AWS Lambda is a serverless compute service that can be used to execute custom code. It can also be used to convert the processed data to a CSV format and generate the metadata XML file. Lambda is a good choice if you need more flexibility in terms of the code that needs to be executed, and if you have smaller volumes of data to process.</p>
<p>In terms of efficiency, Glue is a better choice for larger volumes of data as it can scale horizontally and process data in parallel. It also provides features such as data partitioning, which can further optimize the processing time. However, if you have smaller volumes of data, Lambda can be a more cost-effective solution.</p>
<p>Another option to consider is using a combination of both Glue and Lambda. For example, you could use Glue for the initial conversion of the data to CSV, and then use Lambda to perform additional processing on the data, such as generating the metadata XML file.</p>
<p>### <strong>Glue vs EMR batch/real-time processing</strong></p>
<p>Glue and EMR are two AWS services that can be used to process and analyze data. Each has its own advantages and disadvantages depending on your use case. For your real-time processing and analysis use case for a fleet of automotive sensors in the cloud, here is a comparison between AWS Glue and AWS EMR:</p>
<p><strong>AWS Glue</strong>:</p>
<p>Fully managed Extract, Transform, and Load (ETL) service.</p>
<p>Does not require cluster management or infrastructure configuration.</p>
<p>Billing based on runtime and resources used (pay-as-you-go).</p>
<p>Native support for integration with other AWS services such as S3, Redshift, RDS, etc.</p>
<p>Ability to create ETL tasks based on Apache Spark and Python.</p>
<p>Integrated metadata and data catalog management.</p>
<p>Better suited for batch workloads and ETL tasks.</p>
<p>AWS EMR (Elastic MapReduce):</p>
<p>Fully managed data processing service based on Hadoop and Spark.</p>
<p>More control over cluster configuration and management.</p>
<p>Billing based on EC2 instances used for the cluster and their usage duration.</p>
<p>Support for various data processing frameworks such as Hadoop, Spark, Flink, etc.</p>
<p>Ability to run tasks based on various programming languages (Scala, Java, Python, etc.).</p>
<p>Better suited for real-time workloads, interactive analytics, and complex data processing tasks.</p>
<p>For our specific use case, here are some considerations:</p>
<p>If you prefer a fully managed service with minimal configuration and tight integration with other AWS services, AWS Glue might be a better choice.</p>
<p>If you need real-time analysis and complex data processing with more granular control over cluster management, AWS EMR might be better suited.</p>
<p>Overall, it is important to evaluate your performance, cost, and resource management requirements before choosing between AWS Glue and AWS EMR for your real-time processing and analysis use case for a fleet of automotive sensors in the cloud.</p>
<p>### <strong>Aurora vs Redshift for Data Warehousing</strong></p>
<p><a class="reference external" href="https://hevodata.com/blog/amazon-rds-to-redshift-etl/">https://hevodata.com/blog/amazon-rds-to-redshift-etl/</a></p>
<p><strong>Amazon Aurora:</strong></p>
<p>Est un service de base de données relationnelle compatible avec MySQL et PostgreSQL.
Convient pour les charges de travail transactionnelles (OLTP) avec des requêtes complexes et des opérations de lecture et d'écriture fréquentes.
Offre une haute disponibilité, des sauvegardes automatisées et des réplicas pour une meilleure performance de lecture.
Dans votre cas d'utilisation IoT, Aurora peut être utilisé pour stocker et gérer les données en temps réel provenant de vos capteurs et dispositifs connectés.</p>
<p><strong>Amazon Redshift:</strong></p>
<p>Est un service d'entrepôt de données (data warehouse) conçu pour gérer de grandes quantités de données structurées et semi-structurées.
Optimisé pour les charges de travail analytiques (OLAP) avec des requêtes d'agrégation et de reporting sur de grands ensembles de données.
Intègre de manière transparente avec d'autres services AWS tels que S3, Kinesis, et les outils de visualisation de données comme QuickSight.
Dans votre cas d'utilisation IoT, Redshift peut être utilisé pour stocker, analyser et extraire des informations à partir des données historiques de votre flotte automobile, en fournissant des rapports et des analyses sur les tendances, les performances et les modèles.</p>
<p>Pour notre cas d'utilisation de flotte automobile IoT, il peut être préférable d'utiliser une combinaison d'Aurora et de Redshift. Aurora peut être utilisé pour gérer les données en temps réel et les opérations transactionnelles, tandis que Redshift peut être utilisé pour stocker et analyser les données historiques à des fins d'analyse et de reporting.</p>
<p>## <strong>V. Real-time Analytics</strong></p>
<p>_For real-time analysis, processing and visualization of our vehicle fleets sensors behaviors, the data coming from IoT Core is send to a kinesis streams which then forward the data to Kinesis Analytics with which we can perform real-time data processing with Analytic Studio. Data is then stored to a Timestream Database connected to Quicksight for real-time fleet visualization._</p>
<p>[Tuto : set up streaming etl pipelines with kinesis data analytics](<a class="reference external" href="https://aws.amazon.com/getting-started/hands-on/set-up-streaming-etl-pipelines-apache-flink-and-amazon-kinesis-data-analytics/?ref=gsrchandson">https://aws.amazon.com/getting-started/hands-on/set-up-streaming-etl-pipelines-apache-flink-and-amazon-kinesis-data-analytics/?ref=gsrchandson</a>) _   _</p>
<p>### <em>Advantages of this pipeline</em></p>
<ol class="arabic simple">
<li><p>Direct visualization: The pipeline connects Amazon Timestream to Amazon QuickSight (or Grafana) for real-time visualization, which is more straightforward than using Kinesis Data Firehose and Amazon S3. This makes it easier to create visualizations and dashboards for real-time monitoring of your fleet.</p></li>
<li><p>Simplified data flow: The pipeline eliminates the need for Kinesis Data Firehose and Amazon S3, simplifying the data flow and reducing the number of services involved. This can make the pipeline easier to manage and monitor.</p></li>
<li><p>Optimized for time-series data: Storing processed data in Amazon Timestream is more optimized for time-series data, which is the focus of our use case. Amazon Timestream provides efficient storage and fast querying capabilities for time-series data, making it suitable for the scenario.</p></li>
</ol>
<p>### <em>Potential Improvements</em></p>
<ol class="arabic simple">
<li><p>AWS IoT Analytics: You can use AWS IoT Analytics in combination with AWS IoT Core for a more integrated solution to collect, process, and analyze IoT data. This service can preprocess and filter the incoming data before sending it to Kinesis Data Streams. This allows you to reduce the volume of data being processed downstream and improve the overall efficiency of your pipeline.</p></li>
<li><p>Lambda for data transformation: If you need to perform complex data transformations or generate metadata schema (like XML files) that might not be easily achievable using Kinesis Data Analytics alone, you can introduce AWS Lambda in your pipeline. You can use Lambda to process the data coming from Kinesis Data Streams, apply any necessary transformations, and then send the processed data to Kinesis Data Analytics for further processing.</p></li>
<li><p>Monitoring and alerting: To ensure the reliability and performance of your pipeline, consider implementing monitoring and alerting using Amazon CloudWatch. You can create custom metrics and alarms to track the health of your pipeline components and receive notifications when certain thresholds are exceeded.</p></li>
<li><p>Data retention policy: Since you are using Amazon Timestream for storing processed data, it's essential to define a data retention policy to manage the storage cost and performance. Consider how long you need to retain data at different granularities and configure Timestream accordingly.</p></li>
<li><p>Optimize Kinesis Data Streams: Depending on the volume and velocity of your incoming data, you might need to optimize your Kinesis Data Streams by adjusting the number of shards. Monitor the stream's performance and adjust the shard count as needed to ensure optimal throughput and performance.</p></li>
<li><p>AWS WAF and API Gateway: If you're exposing APIs to interact with your data pipeline, consider implementing AWS WAF (Web Application Firewall) and API Gateway to secure and protect your API endpoints. This will help you manage access control, rate limiting, and protection against common web exploits.</p></li>
<li><p>Auto Scaling: For components in your pipeline that support auto scaling (e.g., Kinesis Data Analytics, AWS Lambda), configure auto scaling policies to ensure your pipeline can handle sudden spikes in data volume or processing requirements without manual intervention.</p></li>
<li><p>Cost optimization: Continuously review and optimize your pipeline's cost by monitoring AWS Cost Explorer and following best practices for each service. For example, review Amazon Timestream's pricing model and adjust data retention policies or use reserved capacity for better cost management.</p></li>
<li><p>Data encryption: Ensure your data is encrypted both in transit and at rest. Enable server-side encryption for Amazon Timestream and Amazon S3. Use AWS Key Management Service (KMS) to manage encryption keys for better security and compliance.</p></li>
<li><p>Automated backups: Implement regular automated backups for critical components in your pipeline, like Amazon Timestream or Amazon S3, to ensure data durability and facilitate disaster recovery.</p></li>
<li><p>Logging and auditing: Enable logging and auditing for all components in your pipeline using AWS CloudTrail and Amazon CloudWatch Logs. Regularly review logs and audit trails to monitor activity, identify potential issues, and maintain compliance with security requirements.</p></li>
</ol>
<p>—-------</p>
<p>### <strong>Kinesis Data Streams</strong></p>
<p>service de flux de données en temps réel qui permet l'ingestion de données en continu, la persistance de données pendant 24 heures ou plus, le traitement de données à l'aide de Lambda ou d'Amazon Kinesis Client Library, et la réplication de données à travers plusieurs régions AWS. Kinesis Data Streams permet également de traiter des données en temps réel à l'aide d'outils tels que Apache Storm, Apache Flink ou Spark Streaming. Il est principalement utilisé pour les cas d'usage nécessitant une faible latence de traitement des données, tels que les analyses de flux de clics, les analyses de fraudes ou les alertes en temps réel.</p>
<p>### <strong>Kinesis Data Firehose</strong></p>
<p>service de livraison de flux de données en temps réel qui permet d'acheminer les données vers des destinations telles qu'Amazon S3, Amazon Redshift, Amazon Elasticsearch ou Amazon Kinesis Data Analytics. Kinesis Data Firehose effectue également des transformations de données en temps réel à l'aide de Lambda ou de logiciels tiers, et prend en charge la réplication de données à travers plusieurs régions AWS. Kinesis Data Firehose est principalement utilisé pour les cas d'usage qui nécessitent un traitement simplifié des données en temps réel, tels que l'archivage de données en temps réel ou l'analyse de logs en temps réel.</p>
<p>En résumé, Kinesis Data Streams est un service de flux de données en temps réel pour l'ingestion, le traitement et la persistance de données en continu, tandis que Kinesis Data Firehose est un service de livraison de flux de données en temps réel pour l'acheminement et la transformation de données vers des destinations de stockage ou d'analyse en temps réel.</p>
<p>### <strong>IoT Analytics vs Kinesis Analytics</strong></p>
<p>[Query Your Data Streams in Real Time With Kinesis Data Analytics Studio](<a class="reference external" href="https://www.youtube.com/watch?v=SX_6x_wXIfA">https://www.youtube.com/watch?v=SX_6x_wXIfA</a>)</p>
<p>[iot-analytics FAQ](<a class="reference external" href="https://www.amazonaws.cn/en/iot-analytics/faq/">https://www.amazonaws.cn/en/iot-analytics/faq/</a>)</p>
<p>_Amazon IoT Analytics <a href="#id11"><span class="problematic" id="id12">FAQ_</span></a></p>
<p>AWS IoT Analytics and Kinesis Analytics are two different services that can be used for processing and analyzing data from IoT devices.</p>
<p>The added value of using AWS IoT Analytics over Kinesis Analytics depends on the specific use case and requirements. Here are some potential advantages of using <strong>AWS IoT Analytics</strong>:</p>
<ul class="simple">
<li><p>Integration with AWS IoT Core: AWS IoT Analytics is designed to integrate seamlessly with AWS IoT Core, the managed cloud service for IoT devices. This integration allows for easy ingestion of data from IoT devices, and makes it possible to enrich, filter, and transform the data using SQL-based queries.</p></li>
<li><p>Pre-built analytics components: AWS IoT Analytics includes pre-built components such as filtering, transformation, and data enrichment that can be used to process IoT data. These components can help to reduce the amount of custom code that needs to be written and simplify the process of building analytics pipelines.</p></li>
<li><p>Data retention: AWS IoT Analytics includes a built-in data retention policy that can be used to specify how long data should be retained. This can help to manage storage costs and simplify data lifecycle management.</p></li>
<li><p>Integration with other AWS services: AWS IoT Analytics integrates with other AWS services such as S3, Kinesis, and Lambda, which can be used to store and process data.</p></li>
</ul>
<p>Overall, AWS IoT Analytics provides a fully managed and integrated solution for processing and analyzing data from IoT devices, with pre-built analytics components, data retention policies, and integration with other AWS services. It can be a good choice for organizations that require an end-to-end solution for IoT data processing and analysis. However, if real-time analytics is a critical requirement, then Kinesis Analytics may be a better choice, as it is designed specifically for real-time streaming data processing and analysis.</p>
<p>### <strong>IoT Events vs IoT Analytics</strong></p>
<p>Both services are provided by AWS to help users to manage and analyze IoT data. However, they have different use cases and functionalities.</p>
<p>IoT Events is a service that helps users to detect and respond to events from their IoT devices in near real-time. It allows you to create rules that match incoming data from your devices and trigger actions based on those rules. IoT Events is mainly used for reactive event processing, anomaly detection, and sending notifications to users or other systems. Here are two use cases for <strong>IoT Events</strong>:</p>
<ol class="arabic simple">
<li><p>Predictive maintenance: Imagine you have a fleet of vehicles, and you want to detect when one of your vehicles has an engine problem before it breaks down. With IoT Events, you can create rules that monitor the data from the vehicle sensors and detect anomalies. For example, if the engine temperature exceeds a certain threshold, you can trigger an event that alerts your maintenance team to take action.</p></li>
<li><p>Security monitoring: You can use IoT Events to monitor your IoT devices for security breaches. For example, you can create a rule that triggers an event when an IoT device tries to connect to an unauthorized network. The event can then be used to send notifications to security teams, block the device's access, or initiate other actions.</p></li>
</ol>
<p>On the other hand, IoT Analytics is a service that allows you to process and analyze large volumes of IoT data. It provides advanced data processing, querying, and visualization capabilities, enabling users to gain insights into their IoT data. Here are two use cases for <strong>IoT Analytics</strong>:</p>
<ol class="arabic simple">
<li><p>Predictive maintenance: Similar to IoT Events, you can use IoT Analytics to monitor your IoT devices for potential maintenance issues. However, with IoT Analytics, you can perform more advanced data processing and analysis. For example, you can use machine learning algorithms to detect patterns in your data that may indicate an impending failure. You can also perform root cause analysis to identify the underlying issues causing the failures.</p></li>
<li><p>Supply chain optimization: Suppose you have a warehouse that receives a large number of shipments each day. With IoT Analytics, you can analyze the data from your sensors and optimize your supply chain operations. For example, you can use historical data to predict the arrival times of your shipments, identify bottlenecks in your warehouse, and optimize your inventory levels.</p></li>
</ol>
<p>In summary, IoT Events is best suited for reactive event processing, anomaly detection, and notifications, while IoT Analytics is better for data processing, analysis, and visualization. Both services can be used together to provide a complete IoT solution.</p>
<p>### <strong>Data Path Kinesis Analytics to Kinesis stream or vise versa ?</strong></p>
<p>data can be sent from Kinesis Streams to Kinesis Analytics and vice versa.</p>
<p>The best way to send data from Kinesis Streams to Kinesis Analytics is to create a Kinesis Analytics application, and then configure the input stream to point to the Kinesis stream where the data is being sent. The Kinesis Analytics application can then use SQL queries to process the data in real-time and generate output data.</p>
<p>To send data from Kinesis Analytics to Kinesis Streams, you can configure the Kinesis Analytics application to generate a stream output. The output stream can be configured to write data to a Kinesis stream or another destination such as S3 or Redshift.</p>
<p>### <strong>Timestream</strong></p>
<p>Timestream is a fully managed time-series database service that is designed to handle large volumes of time-series data with high accuracy and precision.</p>
<p>Timestream provides built-in functions for calculating time-series derivatives such as moving averages, and supports standard SQL queries for data retrieval and analysis. Timestream also provides built-in support for storing and querying metadata, such as tags and custom attributes.</p>
<p>One potential advantage of using Timestream over other databases is its ability to handle high write and query volumes with low latency. Timestream is designed to scale automatically to handle changing data volumes and query patterns, and can support millions of write requests per second.</p>
<p>### <strong>Graphana vs Quicksight</strong></p>
<p>Kinesis Analytics can be integrated with both QuickSight and Grafana for data visualization, and the choice of tool depends on your specific use case and requirements.</p>
<p>QuickSight is a fully managed business intelligence (BI) service that allows you to create interactive dashboards and reports. It provides a drag-and-drop interface for creating visualizations, and supports a wide range of data sources including Kinesis Analytics. QuickSight also provides advanced features such as machine learning-powered insights and natural language queries. QuickSight is a fully managed business intelligence service that is optimized for integration with other AWS services, including Timestream. QuickSight provides built-in integration with Timestream, which enables you to easily connect to and visualize your time-series data in real-time.</p>
<p>Grafana is an open-source platform for data visualization and monitoring. It provides a highly customizable and flexible dashboarding framework, and supports a wide range of data sources including Kinesis Analytics. Grafana also provides features such as alerting, data exploration, and collaboration tools.</p>
<p>## <strong>VI. Machine Learning Pipeline</strong></p>
<p>_The process datas with the time serie features is processed by the ETL (Extract Transform and Load) service AWS Glue in order to be cleaned, converted to csv and xml format and stored to a S3 bucket that will be acessed to the datascientists. The processed datas can finally be used in Sagemaker which allows you to load, processed, train the final data with a model, and host the model with AWS native machine learning tools._</p>
<p>### <strong>Amazon EMR</strong></p>
<p>Managed cluster platform that simplifies running big data frameworks, such as Apache Hadoop and Apache Spark, on AWS to process and analyze vast amounts of data. By using these frameworks and related open-source projects, such as Apache Hive and Apache Pig, you can process data for analytics purposes and business intelligence workloads. Additionally, you can use Amazon EMR to transform and move large amounts of data into and out of other AWS data stores and databases.</p>
<p>Amazon Redshift is the most widely used cloud data warehouse. It makes it fast, simple and cost-effective to analyze all your data using standard SQL and your existing Business Intelligence (BI) tools. It allows you to run complex analytic queries against terabytes to petabytes of structured and semi-structured data, using sophisticated query optimization, columnar storage on high-performance storage, and massively parallel query execution.</p>
<p>Amazon EMR (Elastic MapReduce) on AWS can be used to transform raw JSON files stored in AWS to structured data in CSV format with XML metadata descriptions. EMR is a fully-managed cloud-based big data processing platform that allows users to process large amounts of data using Apache Hadoop, Spark, or other big data processing frameworks.</p>
<p>To transform raw JSON files to CSV with XML metadata, you can use the following steps:</p>
<ol class="arabic simple">
<li><p>Define an XML schema to describe the structure of the JSON data. The schema should define the fields in the JSON data, their types, and any nested or repeated structures.</p></li>
<li><p>Use Amazon S3 to store the JSON data files.</p></li>
<li><p>Create an EMR cluster with the appropriate configuration, including the Hadoop, Spark, or other processing framework of your choice.</p></li>
<li><p>Use a tool like Apache Hive or Apache Pig to read the JSON data files and convert them to structured data in CSV format with the XML metadata.</p></li>
</ol>
<p><strong>Amazon Sagemaker</strong></p>
<p>Amazon SageMaker is a fully managed service that enables you to build, train, and deploy machine learning models quickly. It supports several built-in algorithms and deep learning frameworks that can be used for time series analysis tasks, including anomaly detection and prediction. Here are some of the Amazon SageMaker models and algorithms that are well-suited for time series data:</p>
<ol class="arabic simple">
<li><p>DeepAR: DeepAR is a built-in algorithm in SageMaker that uses recurrent neural networks (RNNs) for time series forecasting. It is designed to handle multiple related time series and can learn to capture complex patterns, trends, and seasonality in the data. DeepAR is suitable for various time series prediction tasks, such as sales forecasting, resource planning, and energy demand prediction.</p></li>
<li><p>Prophet: Although not a built-in algorithm, you can use Facebook's Prophet library with Amazon SageMaker by creating a custom container. Prophet is a powerful time series forecasting tool that is particularly effective at handling data with strong seasonal effects and missing data. It can be used for a wide range of time series prediction tasks.</p></li>
<li><p>Random Cut Forest (RCF): RCF is a built-in unsupervised learning algorithm in SageMaker that can be used for anomaly detection in time series data. It works by building a forest of decision trees and scoring each data point based on its distance from the tree's leaves. RCF can help you identify unusual patterns and outliers in your time series data.</p></li>
<li><p>XGBoost: While XGBoost is primarily known for its use in classification and regression tasks, it can be adapted for time series forecasting by creating appropriate features from the time series data (e.g., lagged values, rolling averages, etc.). You can use the built-in XGBoost algorithm in SageMaker for time series prediction tasks.</p></li>
<li><p>Seq2Seq: Sequence-to-sequence (Seq2Seq) models, based on recurrent neural networks (RNNs) or transformer architectures, can be used for time series forecasting. Although not built-in, you can use TensorFlow or PyTorch within SageMaker to implement custom Seq2Seq models for your time series prediction tasks.</p></li>
<li><p>LSTMs and GRUs: Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) are types of recurrent neural network (RNN) architectures that can be effective for modeling time series data. You can use TensorFlow or PyTorch within SageMaker to implement custom LSTM or GRU models for time series prediction and anomaly detection tasks.</p></li>
<li><p>1D Convolutional Neural Networks (1D CNNs): 1D CNNs can also be used for time series analysis, including forecasting and anomaly detection. You can use TensorFlow or PyTorch within SageMaker to implement custom 1D CNN models tailored to your specific time series tasks.</p></li>
</ol>
<p>In summary, Amazon SageMaker supports several built-in algorithms and deep learning frameworks that can be used for time series analysis tasks, including DeepAR, Prophet, Random Cut Forest, XGBoost, Seq2Seq, LSTMs, GRUs, and 1D CNNs. Depending on your specific requirements, you can choose the most appropriate model or algorithm for your time series prediction or anomaly detection tasks.</p>
<p>## <strong>VII. Sensor Log Analysis and Security  Pipeline</strong></p>
<p>_This pipeline is used to both analyze problems with the IoT sensors itself as well as with the IoT core Service which is the most important component of our infrastructure. Datas coming from IoT Core is passed to IoT Defender, which is stored with cloudwatch logs, then passed to Kinesis firehose to be store in a influxdb database. Logs are processed and store with a Lambda in a S3 bucket. Final datas can be used in Sagemaker._</p>
<p><strong>—-----------------</strong></p>
<ol class="arabic simple">
<li><p>IoT Device Defender is configured to send logs to CloudWatch Logs.</p></li>
<li><p>CloudWatch Logs subscriptions are set up to stream the logs to a Kinesis Data Firehose delivery stream.</p></li>
<li><p>Kinesis Data Firehose delivery stream is configured to transform the incoming logs to a format suitable for storing in the desired database (e.g., Timestream, InfluxDB).</p></li>
<li><p>Kinesis Data Firehose delivery stream delivers the transformed logs to the desired database.</p></li>
<li><p>A Lambda function is triggered by the delivery of new data to the database.</p></li>
<li><p>The Lambda function processes the data to generate the required time series features.</p></li>
<li><p>The Lambda function stores the processed data to the desired format (e.g., CSV) and/or generates the metadata file in XML format.</p></li>
</ol>
<p>### <strong>IoT Defender</strong></p>
<p>AWS IoT Device Defender provides security and compliance monitoring for IoT devices. It allows you to monitor the behavior of your devices, set up rules to detect suspicious activity, and receive alerts when an anomaly is detected. In your use case, you can use IoT Device Defender to monitor the behavior of your connected vehicles and detect any anomalies that could indicate a security breach or malfunction. For example, you can set up rules to detect when a device starts sending data to a suspicious IP address or when it starts transmitting data at an unusually high frequency. When an anomaly is detected, you can receive an alert and take action to investigate and mitigate the issue.</p>
<p>### <strong>Cloudwatch and Cloutrail added value</strong></p>
<p>Enable CloudTrail to log all API calls made to IoT Core by going to the AWS Management Console, selecting the CloudTrail service, and creating a new trail. In the trail settings, select the IoT Core API actions that you want to log.</p>
<ol class="arabic simple">
<li><p>Set up CloudWatch Alarms to monitor for specific events in your IoT Core environment. For example, you could create an alarm to notify you if there is a sudden increase in the number of failed messages from your connected vehicles.</p></li>
<li><p>Create CloudWatch Logs to store the logs generated by CloudTrail. You can configure CloudTrail to deliver the logs directly to CloudWatch Logs, which allows you to view and analyze the logs in near real-time.</p></li>
<li><p>Use CloudWatch Metrics to monitor the health of your IoT Core environment. CloudWatch Metrics provide detailed information on the performance of your IoT Core environment, such as message delivery rates, error rates, and connection rates.</p></li>
</ol>
<p>## <strong>VIII. Consumers, Users, Administrators</strong></p>
<p>_The architecture is managed and used by  three different group : The administrator who open accounts, grants permissions, IAM, roles and manage organisations. Data Scientists who have prenium and exclusive access to the overall infrastructure and the IT Cyber and auditing team which has read access to the IoT Core and Security and log analysis services. Administrator can provide the cyber team with extended access and permissions if deemed necessary._</p>
<p>—-------</p>
<p>### <strong>IAM, Roles, Roles Based Access Control (RBCA), SIngle Sign On (SSO)</strong></p>
<p>To better manage and have a better overview of IAM users and their roles across different AWS services, you can use AWS Organizations, AWS Single Sign-On (SSO), and AWS Identity and Access Management (IAM) with Role-Based Access Control (RBAC).</p>
<p>AWS Organizations allows you to centrally manage and govern multiple AWS accounts. With AWS Organizations, you can create policies that automatically apply to member accounts, such as service control policies (SCPs) to restrict access to specific AWS services.</p>
<p>AWS Single Sign-On (SSO) enables you to centrally manage access to multiple AWS accounts and business applications. With AWS SSO, you can create user groups and assign permissions to these groups using either built-in policies or custom policies. This allows you to easily manage access for different roles and users across different AWS services.</p>
<p>Finally, AWS IAM with RBAC allows you to create roles with specific permissions and assign them to users or groups of users. With IAM, you can also define policies to restrict access to specific actions or resources within a service. IAM also offers the ability to set up multi-factor authentication (MFA) for added security.</p>
<p>By using these services together, you can create a centralized management and governance framework that allows you to easily manage IAM roles and permissions across multiple AWS accounts and services.</p>
<p>## <strong>IX. Continous Integration, Continuous Delivery and Deployment (CICD)</strong></p>
<p>_This pipeline is managed by the datascientist and contains the tools necessary  for collaboration and source code storage (gitlab), deployment automation (Codepipeline) and building and testing the infrastructure (CodeBuild)._</p>
<p>—---------------------------</p>
<p>### <strong>Codebuild, CodePipeline, Gitlab</strong></p>
<p>GitLab, CodePipeline, and CodeBuild can all be used together to create a seamless pipeline for building and deploying applications for IoT Automotive Fleet.</p>
<p>GitLab is a version control system that allows developers to collaborate on code and track changes. It can be used to store code for the IoT Automotive Fleet and manage branches and releases.</p>
<p>CodePipeline is a continuous delivery service that can be used to automate the build, test, and deployment process of the code stored in GitLab. It can also be used to create custom pipelines that automate other processes, such as testing, packaging, and deployment.</p>
<p>CodeBuild is a fully managed build service that compiles source code, runs tests, and produces software packages that can be deployed. It can be integrated with CodePipeline to automatically build and test code changes before deploying them to production.</p>
<ol class="arabic simple">
<li><p>Developers make changes to the code in GitLab.</p></li>
<li><p>CodePipeline is triggered to automatically build and test the code changes.</p></li>
<li><p>CodeBuild compiles the source code, runs tests, and produces a deployable software package.</p></li>
<li><p>CodePipeline deploys the new software package to a test environment for further testing.</p></li>
<li><p>Once the changes have been tested, CodePipeline deploys the software package to production.</p></li>
</ol>
<p>how you could use GitLab, CodePipeline, and CodeBuild together in a coherent way:</p>
<ol class="arabic simple">
<li><p>Create a GitLab repository to store your code.</p></li>
<li><p>Set up a webhook in GitLab to trigger a CodePipeline when a new commit is pushed to the repository.</p></li>
<li><p>Configure your CodePipeline to pull the code from GitLab and run a CodeBuild project.</p></li>
<li><p>Define your CodeBuild project in a buildspec.yml file in the root of your GitLab repository. This file specifies the build commands and environment settings for CodeBuild.</p></li>
<li><dl class="simple">
<dt>an example of how you could integrate Gitlab with AWS CodePipeline:</dt><dd><ol class="arabic simple">
<li><p>Create an S3 bucket:</p></li>
</ol>
</dd>
</dl>
</li>
</ol>
<p><a href="#id3"><span class="problematic" id="id4">``</span></a>`
aws s3api create-bucket --bucket &lt;bucket_name&gt; --region &lt;region&gt; --create-bucket-configuration LocationConstraint=&lt;region&gt;</p>
<p><a href="#id5"><span class="problematic" id="id6">``</span></a><a href="#id7"><span class="problematic" id="id8">`</span></a></p>
<ol class="arabic simple" start="6">
<li><p>This bucket will be used to store the deployment artifacts that are created by AWS CodePipeline.</p></li>
</ol>
<p>7. Create an IAM role: Create an IAM role with the necessary permissions to access the S3 bucket and deploy the application. Assign the role to the CodePipeline service.
8. Create a CodePipeline pipeline: Create a pipeline in CodePipeline that will handle the code deployment process. The pipeline will have three stages: Source, Build, and Deploy. In the Source stage, select GitLab as the source provider and provide the repository information. In the Build stage, select CodeBuild as the build provider and configure the build environment to match your project. In the Deploy stage, select Elastic Beanstalk as the deployment provider and configure the deployment settings.
9. Create a CodeBuild project: Create a CodeBuild project that will build the application artifacts. Configure the project to use GitLab as the source repository and S3 as the artifact store.
10. Create a webhook in GitLab: Create a webhook in GitLab that will notify AWS CodePipeline when changes are made to the repository.</p>
<p>## <strong>X. Durability, fault tolerance, and disaster tolerance</strong></p>
<p>To ensure durability, fault tolerance, and disaster tolerance of S3 buckets, Timestream databases, and overall IoT services infrastructure, several best practices and alternatives can be considered.</p>
<p>For <strong>S3 buckets</strong>, the following approaches can be taken:</p>
<ol class="arabic simple">
<li><p>Cross-Region Replication: This is a feature provided by Amazon S3 to replicate objects across different AWS regions. Cross-Region Replication helps ensure that data is durable and available in the event of a disaster. By replicating the objects across different regions, you can maintain multiple copies of the data and minimize data loss. This approach provides a simple way to replicate S3 buckets across different regions with low latency and high durability.</p></li>
<li><p>Versioning: Enabling versioning on S3 buckets is a way to store multiple versions of an object in the same bucket. This feature helps in case an object is accidentally deleted or modified. With versioning, you can restore the object to a previous version, which helps ensure durability and fault tolerance.</p></li>
<li><p>Lifecycle Policies: This feature allows you to transition objects to different storage classes or delete objects based on predefined rules. This helps reduce the cost of storing objects in S3 and ensures that data is properly managed and retained based on its lifecycle.</p></li>
</ol>
<p>For <strong>Timestream databases</strong>, the following approaches can be taken:</p>
<ol class="arabic simple">
<li><p>Cross-Region Replication: This feature can also be used to replicate Timestream databases across different regions, ensuring that data is durable and available in the event of a disaster. Cross-Region Replication can be configured through AWS Management Console, AWS CLI, or SDKs.</p></li>
<li><p>Backup and Restore: Timestream provides a backup and restore feature that can be used to create backups of Timestream tables and databases. Backups can be used to restore data in case of accidental deletion or modification. This approach helps ensure durability, fault tolerance, and disaster tolerance.</p></li>
<li><p>Data Retention Policies: Timestream allows you to define data retention policies to automatically delete old data. This helps reduce the cost of storing data in Timestream and ensures that data is properly managed based on its lifecycle.</p></li>
</ol>
<p>For overall IoT services infrastructure, the following approaches can be taken:</p>
<ol class="arabic simple">
<li><p>Multi-AZ Deployment: Deploying your IoT services in multiple Availability Zones (AZs) helps ensure high availability and fault tolerance. Multi-AZ deployment ensures that if one AZ goes down, the service will continue to function from another AZ.</p></li>
<li><p>Disaster Recovery: Disaster Recovery (DR) solutions can be used to replicate data and applications to a secondary location to ensure business continuity in the event of a disaster. AWS provides several DR solutions such as AWS Backup, AWS Site-to-Site VPN, and AWS Direct Connect.</p></li>
<li><p>Auto Scaling: Auto Scaling can be used to automatically adjust the capacity of your IoT services based on the demand. This helps ensure that your services can handle sudden spikes in traffic or demand, which helps ensure high availability and fault tolerance.</p></li>
</ol>
<p>In conclusion, to ensure durability, fault tolerance, and disaster tolerance of S3 buckets, Timestream databases, and overall IoT services infrastructure, various approaches can be taken. The best approach depends on the specific requirements of the use case, such as data size, frequency of updates, and access patterns. By choosing the appropriate approach, organizations can ensure that their data and services are always available, durable, and fault-tolerant.</p>
<p><strong>TO DO</strong></p>
<p>Titre introductif sur l’architecture</p>
<p>AWS IoT Device Management</p>
<p>Batch analytics / IoT Defenders and Logs bien revoir la pipeline</p>
<p>Déployer les modèles on edge/lambda</p>
<p>IoT Greengrass vs IoT Core vs AppSync</p>
<p>Protocoles de communication</p>
<p>Bien revoir GLUE / Glue vs Step functions / Glue vs EMR [glue-VS-emr](<a class="reference external" href="https://www.trianz.com/insights/aws-glue-vs-emr">https://www.trianz.com/insights/aws-glue-vs-emr</a>)</p>
<p>Codepipeline/Codebuild</p>
<p>SNS Notification to CYBER Teams</p>
<p>Cost Surveillance mechanisms</p>
<p>CLoudwatch AWS Ressource Management / Cloudwatch/vs/Cloudtrail</p>
<p>QuickSight/vs/Grafana</p>
<p>Timestream/vs/InfluxDB/vs/DynamoDB/vs/TimescaleDB</p>
<p>Security and log analysis pipeline : AWS Systems Manager Parameter Store vs Timestram for storing logs ?</p>
<p>Pour le la pipeline batch et cold storage/analysis ===&gt; why not using lambda rather than EMR or athena</p>
<p>and storing processed query to dynamo DB ===&gt; answer : to moo datas for lambda but athena could be used on our redshift datawarehouse</p>
<p>IoT Analytics vs Kinesis Analytics</p>
<p>AWS Certificate Manager vs AWS Private CA</p>
<p>Aurora vs Redshift</p>
<p>Why not storing time series directly from Fleetwise to TimestreamDB ? Too high volume, too     much vehicles, we want very high frequency raws datas, timestream will be too messy.</p>
<p>Backup, data security, storage, replication ….</p>
<p>Load balancing, autoscaling the services (Firehorse, Streams etc …)</p>
<p>Security Like VPC, subnet, gateway for Services</p>
<p>IAM, AWS Certificate Management etc …</p>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="#">Automotive IoT Sensor Fleet Deployment on AWS : An Architecture Proposal and Documentation</a></h1>








<h3>Navigation</h3>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="#">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2023, Clément Siegrist.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 5.0.2</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="_sources/index.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>